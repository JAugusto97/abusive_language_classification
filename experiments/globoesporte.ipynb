{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo: generate bigrams to evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix, make_scorer\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data/youtube_text.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comentario</th>\n",
       "      <th>toxico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27609</td>\n",
       "      <td>O povo que foi as ruas devem permanecerem unid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44934</td>\n",
       "      <td>Concordo contigo, por√©m, √© sempre bom lembrar ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19854</td>\n",
       "      <td>Henry Bugalho maior lixo do YouTube.  Mau car√°...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7392</td>\n",
       "      <td>Essa cidad√£ do PSOL √â A MAIOR CRETINA EM SOLO ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3547</td>\n",
       "      <td>Rapa no zero, deixa a barbona de viking, malha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16917</td>\n",
       "      <td>Mam√£e chupei .... por qu√™ n√£o se revoltam conF...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44405</td>\n",
       "      <td>TNC mbl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62222</td>\n",
       "      <td>nego reclamando que nao teve repercussao no ES...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42319</td>\n",
       "      <td>Pior que √© vdd ! Minha m√£e at√© ontem estava ac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>74672</td>\n",
       "      <td>Mas o \"hacker\" n√£o era o russo?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37047</td>\n",
       "      <td>O IDIOTA...falando pros OTARIOS...N√ìS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20244</td>\n",
       "      <td>Um conselho amigo n√£o vai nesse de bate com o ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>74052</td>\n",
       "      <td>Meu querido  . . . \\nN√ÉO CAI A FICHA. \\nEu gos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13655</td>\n",
       "      <td>Oooooooohh BICHA!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45383</td>\n",
       "      <td>Algu√©m ser√° q poderia cortar a l√≠ngua dessa ro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>76398</td>\n",
       "      <td>O Brasil precisa de um minist√©rio formado das ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20468</td>\n",
       "      <td>ARTHUR, POR FAVOR, D√Å UMA AJUDA NO CASO DO PAU...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30917</td>\n",
       "      <td>Sou do Cear√° ,mas voto sempre 17171717....‚Ä¶</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22740</td>\n",
       "      <td>Esse video tem que chegar com urgencia ao mini...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>43781</td>\n",
       "      <td>üëÄüëçüèº‚ú®üáßüá∑  Valeu RENAN !!!!!  Tontos mas fora Pt ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16572</td>\n",
       "      <td>O Arthur claramente ficava feliz quando era qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>42840</td>\n",
       "      <td>Esse senhor ai astrologo, s√≥ engana molequinho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8671</td>\n",
       "      <td>Seguran√ßa armado s√≥ pra quem √© muito rico. Que...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11676</td>\n",
       "      <td>At√© o Spark Mandrill sabe que a previd√™ncia va...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>33399</td>\n",
       "      <td>Bichona</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20513</td>\n",
       "      <td>Gostaria de ver o v√≠deo:\\n\"Ousei criticar o mb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21812</td>\n",
       "      <td>Que sotaque nojento esse sotaque carioca... pq...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>48013</td>\n",
       "      <td>Maria do Ros√°rio na cadeia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>337</td>\n",
       "      <td>*Eu com 17 anos estava na manifesta√ß√£o que oco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>53680</td>\n",
       "      <td>Canal mongol√≥ide</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72561</th>\n",
       "      <td>164</td>\n",
       "      <td>A direita t√™m que se unir. A direita √© a maior...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72761</th>\n",
       "      <td>472</td>\n",
       "      <td>Tem ser preso a bicha.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72981</th>\n",
       "      <td>468</td>\n",
       "      <td>Foga√ßa √© uma bicha da turminha da lacra√ß√£o. \\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73033</th>\n",
       "      <td>409</td>\n",
       "      <td>Eu tomei tomei um hj por chamar um cara de via...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73076</th>\n",
       "      <td>120</td>\n",
       "      <td>*BRASIL EM UM PA√çS QUE IMPRESSIONA \"AQUI PAV√ÉO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73098</th>\n",
       "      <td>459</td>\n",
       "      <td>Esse pavao √© um adolescente punheteiro e esse ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73159</th>\n",
       "      <td>330</td>\n",
       "      <td>WATCHDOGS DO BRASIL! A PF PRENDEU O HACKER \"DE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74057</th>\n",
       "      <td>461</td>\n",
       "      <td>Se o moro n√£o falou o que dizem que falou, ele...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74142</th>\n",
       "      <td>170</td>\n",
       "      <td>Eu sigo o Nando antes mesmo dele ter alcan√ßado...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74256</th>\n",
       "      <td>69</td>\n",
       "      <td>Abro o v√≠deo do Nando e me deparo com uma prop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74274</th>\n",
       "      <td>264</td>\n",
       "      <td>Vesti minha camisa do Brasil, sai de casa fui ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74618</th>\n",
       "      <td>245</td>\n",
       "      <td>\"Nunca apagaras as conversas do Whatsapp, e ai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74657</th>\n",
       "      <td>204</td>\n",
       "      <td>E o PALHA√áO TIRIRICA foi o √∫nico de seu partid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74866</th>\n",
       "      <td>175</td>\n",
       "      <td>O Edgard √© um bosta, a m√°scara terminou se cai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75357</th>\n",
       "      <td>158</td>\n",
       "      <td>Pedro Bial √© um esquerdista idiota e veado e m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75629</th>\n",
       "      <td>213</td>\n",
       "      <td>kkkkk, Nando Moura vc √© muito engra√ßado. Obrig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75787</th>\n",
       "      <td>335</td>\n",
       "      <td>Faz um v√≠deo falando do viadao do Glen Bosta</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75844</th>\n",
       "      <td>125</td>\n",
       "      <td>Fala a√≠ pro teus seguidores,que o Freixo coloc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76032</th>\n",
       "      <td>374</td>\n",
       "      <td>√â Natal √â Natal mama o meu pau</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76044</th>\n",
       "      <td>30</td>\n",
       "      <td>Nando, aqui no Rio a gente chama de \"para√≠ba\" ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76194</th>\n",
       "      <td>422</td>\n",
       "      <td>Lulu Santos viadao da porra.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76413</th>\n",
       "      <td>367</td>\n",
       "      <td>o viadao, fala da investigacao na puta do bols...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76459</th>\n",
       "      <td>13</td>\n",
       "      <td>vai cuntinuar sendo viado .....a coseira na bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76532</th>\n",
       "      <td>160</td>\n",
       "      <td>Jean wiliss n√£o √© l√≠der nem do pr√≥prio cu , t√°...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76806</th>\n",
       "      <td>312</td>\n",
       "      <td>Que propaganda chata essa do trivago \\n\\nEsse ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76814</th>\n",
       "      <td>156</td>\n",
       "      <td>Eu n√£o vou assistir os Vingadores,porque o Lul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77000</th>\n",
       "      <td>354</td>\n",
       "      <td>Claro que n√£o o bolsonaro nem mascara tinha el...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77045</th>\n",
       "      <td>262</td>\n",
       "      <td>N√£o podemos abandonar o Homem aos C√£es. \\nOrem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77099</th>\n",
       "      <td>395</td>\n",
       "      <td>Ela √© funcion√°ria da Globo.\\nSegue programa√ß√£o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77637</th>\n",
       "      <td>2</td>\n",
       "      <td>Tem que ser gordo viado  ningu√©m pergunto vc n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>913 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                         comentario toxico\n",
       "0           27609  O povo que foi as ruas devem permanecerem unid...      0\n",
       "1           44934  Concordo contigo, por√©m, √© sempre bom lembrar ...      0\n",
       "2           19854  Henry Bugalho maior lixo do YouTube.  Mau car√°...      1\n",
       "3            7392  Essa cidad√£ do PSOL √â A MAIOR CRETINA EM SOLO ...      1\n",
       "4            3547  Rapa no zero, deixa a barbona de viking, malha...      0\n",
       "5           16917  Mam√£e chupei .... por qu√™ n√£o se revoltam conF...      1\n",
       "6           44405                                            TNC mbl      1\n",
       "7           62222  nego reclamando que nao teve repercussao no ES...      1\n",
       "8           42319  Pior que √© vdd ! Minha m√£e at√© ontem estava ac...      0\n",
       "9           74672                    Mas o \"hacker\" n√£o era o russo?      0\n",
       "10          37047           O IDIOTA...falando pros OTARIOS...N√ìS...      1\n",
       "11          20244  Um conselho amigo n√£o vai nesse de bate com o ...      1\n",
       "12          74052  Meu querido  . . . \\nN√ÉO CAI A FICHA. \\nEu gos...      0\n",
       "13          13655                                Oooooooohh BICHA!!!      0\n",
       "14          45383  Algu√©m ser√° q poderia cortar a l√≠ngua dessa ro...      1\n",
       "15          76398  O Brasil precisa de um minist√©rio formado das ...      1\n",
       "16          20468  ARTHUR, POR FAVOR, D√Å UMA AJUDA NO CASO DO PAU...      0\n",
       "17          30917        Sou do Cear√° ,mas voto sempre 17171717....‚Ä¶      0\n",
       "18          22740  Esse video tem que chegar com urgencia ao mini...      0\n",
       "19          43781  üëÄüëçüèº‚ú®üáßüá∑  Valeu RENAN !!!!!  Tontos mas fora Pt ...      0\n",
       "20          16572  O Arthur claramente ficava feliz quando era qu...      0\n",
       "21          42840  Esse senhor ai astrologo, s√≥ engana molequinho...      1\n",
       "22           8671  Seguran√ßa armado s√≥ pra quem √© muito rico. Que...      1\n",
       "23          11676  At√© o Spark Mandrill sabe que a previd√™ncia va...      1\n",
       "24          33399                                            Bichona      0\n",
       "25          20513  Gostaria de ver o v√≠deo:\\n\"Ousei criticar o mb...      0\n",
       "26          21812  Que sotaque nojento esse sotaque carioca... pq...      1\n",
       "27          48013                         Maria do Ros√°rio na cadeia      0\n",
       "28            337  *Eu com 17 anos estava na manifesta√ß√£o que oco...      0\n",
       "29          53680                                   Canal mongol√≥ide      1\n",
       "...           ...                                                ...    ...\n",
       "72561         164  A direita t√™m que se unir. A direita √© a maior...      1\n",
       "72761         472                             Tem ser preso a bicha.      1\n",
       "72981         468  Foga√ßa √© uma bicha da turminha da lacra√ß√£o. \\n...      1\n",
       "73033         409  Eu tomei tomei um hj por chamar um cara de via...      0\n",
       "73076         120  *BRASIL EM UM PA√çS QUE IMPRESSIONA \"AQUI PAV√ÉO...      0\n",
       "73098         459  Esse pavao √© um adolescente punheteiro e esse ...      1\n",
       "73159         330  WATCHDOGS DO BRASIL! A PF PRENDEU O HACKER \"DE...      1\n",
       "74057         461  Se o moro n√£o falou o que dizem que falou, ele...      0\n",
       "74142         170  Eu sigo o Nando antes mesmo dele ter alcan√ßado...      0\n",
       "74256          69  Abro o v√≠deo do Nando e me deparo com uma prop...      1\n",
       "74274         264  Vesti minha camisa do Brasil, sai de casa fui ...      1\n",
       "74618         245  \"Nunca apagaras as conversas do Whatsapp, e ai...      0\n",
       "74657         204  E o PALHA√áO TIRIRICA foi o √∫nico de seu partid...      0\n",
       "74866         175  O Edgard √© um bosta, a m√°scara terminou se cai...      1\n",
       "75357         158  Pedro Bial √© um esquerdista idiota e veado e m...      1\n",
       "75629         213  kkkkk, Nando Moura vc √© muito engra√ßado. Obrig...      0\n",
       "75787         335       Faz um v√≠deo falando do viadao do Glen Bosta      1\n",
       "75844         125  Fala a√≠ pro teus seguidores,que o Freixo coloc...      0\n",
       "76032         374                     √â Natal √â Natal mama o meu pau      1\n",
       "76044          30  Nando, aqui no Rio a gente chama de \"para√≠ba\" ...      1\n",
       "76194         422                       Lulu Santos viadao da porra.      1\n",
       "76413         367  o viadao, fala da investigacao na puta do bols...      1\n",
       "76459          13  vai cuntinuar sendo viado .....a coseira na bu...      1\n",
       "76532         160  Jean wiliss n√£o √© l√≠der nem do pr√≥prio cu , t√°...      1\n",
       "76806         312  Que propaganda chata essa do trivago \\n\\nEsse ...      1\n",
       "76814         156  Eu n√£o vou assistir os Vingadores,porque o Lul...      0\n",
       "77000         354  Claro que n√£o o bolsonaro nem mascara tinha el...      1\n",
       "77045         262  N√£o podemos abandonar o Homem aos C√£es. \\nOrem...      1\n",
       "77099         395  Ela √© funcion√°ria da Globo.\\nSegue programa√ß√£o...      1\n",
       "77637           2  Tem que ser gordo viado  ningu√©m pergunto vc n...      1\n",
       "\n",
       "[913 rows x 3 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/corpus/OffComBR/OffComBR3.arff\") as f:\n",
    "    dataDictionary = arff.load(f)\n",
    "    f.close()\n",
    "    \n",
    "globoesporte_data = pd.DataFrame(dataDictionary['data'], columns=['toxico', 'comentario'])\n",
    "globoesporte_data['toxico'] = globoesporte_data['toxico'].apply(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxico</th>\n",
       "      <th>comentario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Votaram no PEZAO Agora tomem no CZAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>cuidado com a poupanca pessoal Lembram o que a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Sabe o que eu acho engracado os nossos governa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Podiam retirar dos lucros dos bancos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>CADE O GALVAO PRA NARRAR AGORA   FALIIIIUUUUUU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Nao e possivel que deputados tenham tantos car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Sejam honestos aprovem o projeto original vamo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Nao a pizza anunciada de anistia do caixa  Sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>quem ja viu bandidos fazendo leis contra antic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Olha os caras fazendo algo de util e de intere...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxico                                         comentario\n",
       "0       1               Votaram no PEZAO Agora tomem no CZAO\n",
       "1       0  cuidado com a poupanca pessoal Lembram o que a...\n",
       "2       0  Sabe o que eu acho engracado os nossos governa...\n",
       "3       0              Podiam retirar dos lucros dos bancos \n",
       "4       0  CADE O GALVAO PRA NARRAR AGORA   FALIIIIUUUUUU...\n",
       "5       0  Nao e possivel que deputados tenham tantos car...\n",
       "6       0  Sejam honestos aprovem o projeto original vamo...\n",
       "7       0  Nao a pizza anunciada de anistia do caixa  Sim...\n",
       "8       0  quem ja viu bandidos fazendo leis contra antic...\n",
       "9       0  Olha os caras fazendo algo de util e de intere..."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globoesporte_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxico</th>\n",
       "      <th>comentario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Votaram no PEZAO Agora tomem no CZAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>Voces sao idiotas ou se fazem Voces sabem que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>Alexandre Acioni vai estudar pra saber o que e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>Mais um pobre metido a besta ja ja fica sem di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>Martin Sales deixa de ser idiot  agora ninguem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>nem sou mas esse cara e um velho asqueroso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>A FMI nao manda nas economia mundiais Que vao ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>PAULO VOCE E BURRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>VAO BATE PANELASSEUS BURROSBEM FEITO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>ninguem se importa com vc nem sua mae seu nada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     toxico                                         comentario\n",
       "0         1               Votaram no PEZAO Agora tomem no CZAO\n",
       "25        1  Voces sao idiotas ou se fazem Voces sabem que ...\n",
       "31        1  Alexandre Acioni vai estudar pra saber o que e...\n",
       "42        1  Mais um pobre metido a besta ja ja fica sem di...\n",
       "46        1  Martin Sales deixa de ser idiot  agora ninguem...\n",
       "60        1         nem sou mas esse cara e um velho asqueroso\n",
       "66        1  A FMI nao manda nas economia mundiais Que vao ...\n",
       "83        1                                 PAULO VOCE E BURRO\n",
       "84        1               VAO BATE PANELASSEUS BURROSBEM FEITO\n",
       "148       1     ninguem se importa com vc nem sua mae seu nada"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globoesporte_data[globoesporte_data['toxico'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating different classifiers with different features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words (baseline approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining possible parameters for every classifier\n",
    "SVM = {'name': 'svm',\n",
    "       'classifier': SVC(),\n",
    "       'parameters': {'kernel': ['linear', 'rbf'],\n",
    "       'C': [1.0, 5, 10, 15],\n",
    "       'gamma': [0.1, 0.5, 1.0, 5.0, 10]}}\n",
    "\n",
    "Logistic = {'name': 'logistic_regression',\n",
    "            'classifier': LogisticRegression(),\n",
    "            'parameters': {'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1.0, 5.0, 10],\n",
    "            'max_iter': [100, 500, 1000],\n",
    "            'solver': ['liblinear']}}\n",
    "\n",
    "MLP = {'name': 'multilayer_perceptron',\n",
    "       'classifier': MLPClassifier(),\n",
    "       'parameters': {'hidden_layer_sizes':[(100,),(1000,),(100,100)],\n",
    "                      'solver': ['lbfgs']}}\n",
    "\n",
    "Random_Forest = {'name': 'random_forest',\n",
    "                 'classifier': RandomForestClassifier(),\n",
    "                 'parameters': {'n_estimators': [10, 50, 100, 300, 'warn']}}\n",
    "\n",
    "classifiers = [SVM, Logistic, MLP, Random_Forest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CountVectorizer().fit_transform(globoesporte_data['comentario'])\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   48.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  25 | elapsed:    3.2s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    7.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=True,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.7641084696861176\n",
      "logistic_regression: 0.7824183024958901\n",
      "multilayer_perceptron: 0.7624698614065534\n",
      "random_forest: 0.7751969749123615\n"
     ]
    }
   ],
   "source": [
    "bow_results = {}\n",
    "for name, model in models.items():\n",
    "    bow_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring=make_scorer(f1_score, average='weighted')))\n",
    "    \n",
    "for name, result in bow_results.items():\n",
    "    print(name+':', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      " [[719 112]\n",
      " [128  74]]\n",
      "logistic_regression\n",
      " [[775  56]\n",
      " [145  57]]\n",
      "multilayer_perceptron\n",
      " [[703 128]\n",
      " [121  81]]\n",
      "random_forest\n",
      " [[788  43]\n",
      " [156  46]]\n"
     ]
    }
   ],
   "source": [
    "bow_confusion_matrix = {}\n",
    "for name, model in models.items():\n",
    "    y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "    bow_confusion_matrix[name] = confusion_matrix(y, y_pred)\n",
    "    print(name+'\\n',bow_confusion_matrix[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf - idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining possible parameters for every classifier\n",
    "SVM = {'name': 'svm',\n",
    "       'classifier': SVC(),\n",
    "       'parameters': {'kernel': ['linear', 'rbf'],\n",
    "       'C': [1.0, 5, 10, 15],\n",
    "       'gamma': [0.1, 0.5, 1.0, 5.0, 10]}}\n",
    "\n",
    "Logistic = {'name': 'logistic_regression',\n",
    "            'classifier': LogisticRegression(),\n",
    "            'parameters': {'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1.0, 5.0, 10],\n",
    "            'max_iter': [100, 500, 1000],\n",
    "            'solver': ['liblinear']}}\n",
    "\n",
    "MLP = {'name': 'multilayer_perceptron',\n",
    "       'classifier': MLPClassifier(),\n",
    "       'parameters': {'hidden_layer_sizes':[(100,),(1000,),(100,100)],\n",
    "                      'solver': ['lbfgs']}}\n",
    "\n",
    "Random_Forest = {'name': 'random_forest',\n",
    "                 'classifier': RandomForestClassifier(),\n",
    "                 'parameters': {'n_estimators': [10, 50, 100, 300, 'warn']}}\n",
    "\n",
    "classifiers = [SVM, Logistic, MLP, Random_Forest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer().fit(globoesporte_data['comentario'])\n",
    "X = vectorizer.transform(globoesporte_data['comentario'])\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=0,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.7821448858374002\n",
      "logistic_regression: 0.7703214605694468\n",
      "multilayer_perceptron: 0.7706355273869947\n",
      "random_forest: 0.7629019537879375\n"
     ]
    }
   ],
   "source": [
    "tf_idf_results = {}\n",
    "for name, model in models.items():\n",
    "    tf_idf_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring=make_scorer(f1_score, average='weighted')))\n",
    "    \n",
    "for name, result in tf_idf_results.items():\n",
    "    print(name+':', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      " [[783  48]\n",
      " [149  53]]\n",
      "logistic_regression\n",
      " [[806  25]\n",
      " [167  35]]\n",
      "multilayer_perceptron\n",
      " [[731 100]\n",
      " [134  68]]\n",
      "random_forest\n",
      " [[780  51]\n",
      " [161  41]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_confusion_matrix = {}\n",
    "for name, model in models.items():\n",
    "    y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "    tf_idf_confusion_matrix[name] = confusion_matrix(y, y_pred)\n",
    "    print(name+'\\n',tf_idf_confusion_matrix[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embeddings with size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining possible parameters for every classifier\n",
    "SVM = {'name': 'svm',\n",
    "       'classifier': SVC(),\n",
    "       'parameters': {'kernel': ['linear', 'rbf'],\n",
    "       'C': [1.0, 5, 10, 15],\n",
    "       'gamma': [0.1, 0.5, 1.0, 5.0, 10]}}\n",
    "\n",
    "Logistic = {'name': 'logistic_regression',\n",
    "            'classifier': LogisticRegression(),\n",
    "            'parameters': {'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1.0, 5.0, 10],\n",
    "            'max_iter': [100, 500, 1000],\n",
    "            'solver': ['liblinear']}}\n",
    "\n",
    "MLP = {'name': 'multilayer_perceptron',\n",
    "       'classifier': MLPClassifier(),\n",
    "       'parameters': {'hidden_layer_sizes':[(100,),(1000,),(100,100)],\n",
    "                      'solver': ['lbfgs']}}\n",
    "\n",
    "Random_Forest = {'name': 'random_forest',\n",
    "                 'classifier': RandomForestClassifier(),\n",
    "                 'parameters': {'n_estimators': [10, 50, 100, 300, 'warn']}}\n",
    "\n",
    "classifiers = [SVM, Logistic, MLP, Random_Forest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating array of tokens for every comment\n",
    "comments = []\n",
    "STOPWORDS = nltk.corpus.stopwords.words('portuguese')\n",
    "for i, comment in globoesporte_data['comentario'].iteritems():\n",
    "    tokenized_comment = nltk.word_tokenize(comment.lower())\n",
    "    \n",
    "    clean_comment = [token for token in tokenized_comment if\n",
    "                     len(token) > 3 and\n",
    "                     token not in STOPWORDS and\n",
    "                     token.isalpha()]\n",
    "    \n",
    "    comments.append(clean_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_cbow = gensim.models.Word2Vec.load('data/word_embeddings/twitter_cbow_100_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging vectors of tokens into vectors of comments\n",
    "not_found = []\n",
    "found = 0\n",
    "comments_vectors = []\n",
    "for comment in comments:\n",
    "    comment_vector = np.zeros((100))\n",
    "    n_tokens = 1\n",
    "    for token in comment:\n",
    "        n_tokens += 1\n",
    "        try:\n",
    "            comment_vector += twitter_cbow.wv.get_vector(token)\n",
    "            found += 1\n",
    "        except KeyError:\n",
    "            comment_vector += np.zeros((100))\n",
    "            not_found.append(token)\n",
    "            \n",
    "    comments_vectors.append(comment_vector/n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments_vectors\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   23.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   28.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    8.6s finished\n"
     ]
    }
   ],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=True,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.8035218360089497\n",
      "logistic_regression: 0.7989817911050029\n",
      "multilayer_perceptron: 0.7427340815701806\n",
      "random_forest: 0.7875912494377239\n"
     ]
    }
   ],
   "source": [
    "twitter_cbow_results = {}\n",
    "for name, model in models.items():\n",
    "    twitter_cbow_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring=make_scorer(f1_score, average='weighted')))\n",
    "    \n",
    "for name, result in twitter_cbow_results.items():\n",
    "    print(name+':', result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      " [[771  60]\n",
      " [126  76]]\n",
      "logistic_regression\n",
      " [[747  84]\n",
      " [114  88]]\n",
      "multilayer_perceptron\n",
      " [[692 139]\n",
      " [119  83]]\n",
      "random_forest\n",
      " [[809  22]\n",
      " [161  41]]\n"
     ]
    }
   ],
   "source": [
    "twitter_cbow_confusion_matrix = {}\n",
    "for name, model in models.items():\n",
    "    y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "    twitter_cbow_confusion_matrix[name] = confusion_matrix(y, y_pred)\n",
    "    print(name+'\\n',twitter_cbow_confusion_matrix[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NILC Cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining possible parameters for every classifier\n",
    "SVM = {'name': 'svm',\n",
    "       'classifier': SVC(),\n",
    "       'parameters': {'kernel': ['linear', 'rbf'],\n",
    "       'C': [1.0, 5, 10, 15],\n",
    "       'gamma': [0.1, 0.5, 1.0, 5.0, 10]}}\n",
    "\n",
    "Logistic = {'name': 'logistic_regression',\n",
    "            'classifier': LogisticRegression(),\n",
    "            'parameters': {'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1.0, 5.0, 10],\n",
    "            'max_iter': [100, 500, 1000],\n",
    "            'solver': ['liblinear']}}\n",
    "\n",
    "MLP = {'name': 'multilayer_perceptron',\n",
    "       'classifier': MLPClassifier(),\n",
    "       'parameters': {'hidden_layer_sizes':[(100,),(1000,),(100,100)],\n",
    "                      'solver': ['lbfgs']}}\n",
    "\n",
    "Random_Forest = {'name': 'random_forest',\n",
    "                 'classifier': RandomForestClassifier(),\n",
    "                 'parameters': {'n_estimators': [10, 50, 100, 300, 'warn']}}\n",
    "\n",
    "classifiers = [SVM, Logistic, MLP, Random_Forest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "nilc_cbow = KeyedVectors.load_word2vec_format('data/word_embeddings/cbow_s100.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging vectors of tokens into vectors of comments\n",
    "not_found = []\n",
    "found = 0\n",
    "comments_vectors = []\n",
    "for comment in comments:\n",
    "    comment_vector = np.zeros((100))\n",
    "    n_tokens = 1\n",
    "    for token in comment:\n",
    "        n_tokens += 1\n",
    "        try:\n",
    "            comment_vector += nilc_cbow.wv.get_vector(token)\n",
    "            found += 1\n",
    "        except KeyError:\n",
    "            comment_vector += np.zeros((100))\n",
    "            not_found.append(token)\n",
    "            \n",
    "    comments_vectors.append(comment_vector/n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments_vectors\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=0,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.8207724462750393\n",
      "logistic_regression: 0.8181598602562492\n",
      "multilayer_perceptron: 0.7648927828816331\n",
      "random_forest: 0.7876024395669415\n"
     ]
    }
   ],
   "source": [
    "nilc_cbow_results = {}\n",
    "for name, model in models.items():\n",
    "    nilc_cbow_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring=make_scorer(f1_score, average='weighted')))\n",
    "    \n",
    "for name, result in nilc_cbow_results.items():\n",
    "    print(name+':', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      " [[771  60]\n",
      " [114  88]]\n",
      "logistic_regression\n",
      " [[772  59]\n",
      " [117  85]]\n",
      "multilayer_perceptron\n",
      " [[707 124]\n",
      " [113  89]]\n",
      "random_forest\n",
      " [[818  13]\n",
      " [162  40]]\n"
     ]
    }
   ],
   "source": [
    "nilc_cbow_confusion_matrix = {}\n",
    "for name, model in models.items():\n",
    "    y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "    nilc_cbow_confusion_matrix[name] = confusion_matrix(y, y_pred)\n",
    "    print(name+'\\n',nilc_cbow_confusion_matrix[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining possible parameters for every classifier\n",
    "SVM = {'name': 'svm',\n",
    "       'classifier': SVC(),\n",
    "       'parameters': {'kernel': ['linear', 'rbf'],\n",
    "       'C': [1.0, 5, 10, 15],\n",
    "       'gamma': [0.1, 0.5, 1.0, 5.0, 10]}}\n",
    "\n",
    "Logistic = {'name': 'logistic_regression',\n",
    "            'classifier': LogisticRegression(),\n",
    "            'parameters': {'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1.0, 5.0, 10],\n",
    "            'max_iter': [100, 500, 1000],\n",
    "            'solver': ['liblinear']}}\n",
    "\n",
    "MLP = {'name': 'multilayer_perceptron',\n",
    "       'classifier': MLPClassifier(),\n",
    "       'parameters': {'hidden_layer_sizes':[(100,),(1000,),(100,100)],\n",
    "                      'solver': ['lbfgs']}}\n",
    "\n",
    "Random_Forest = {'name': 'random_forest',\n",
    "                 'classifier': RandomForestClassifier(),\n",
    "                 'parameters': {'n_estimators': [10, 50, 100, 300, 'warn']}}\n",
    "\n",
    "classifiers = [SVM, Logistic, MLP, Random_Forest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_skipgram = gensim.models.Word2Vec.load('data/word_embeddings/twitter_skipgram_100_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_vectors = []\n",
    "for comment in comments:\n",
    "    comment_vector = np.zeros((100))\n",
    "    if not comment:\n",
    "        comments_vectors.append(comment_vector)\n",
    "    else:\n",
    "        n_tokens = 0\n",
    "        for token in comment:\n",
    "            n_tokens += 1\n",
    "            try:\n",
    "                comment_vector += twitter_skipgram.wv.get_vector(token)\n",
    "            except KeyError:\n",
    "                comment_vector += np.zeros((100))\n",
    "            \n",
    "        comments_vectors.append(comment_vector/n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments_vectors\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   22.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    8.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   18.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    9.8s finished\n"
     ]
    }
   ],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=2,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.82062913386935\n",
      "logistic_regression: 0.8301687738470237\n",
      "multilayer_perceptron: 0.7702263025770355\n",
      "random_forest: 0.7691131811709627\n"
     ]
    }
   ],
   "source": [
    "twitter_sg_results = {}\n",
    "for name, model in models.items():\n",
    "    twitter_sg_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring=make_scorer(f1_score, average='weighted')))\n",
    "    \n",
    "for name, result in twitter_sg_results.items():\n",
    "    print(name+':', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      " [[786  45]\n",
      " [122  80]]\n",
      "logistic_regression\n",
      " [[775  56]\n",
      " [109  93]]\n",
      "multilayer_perceptron\n",
      " [[688 143]\n",
      " [108  94]]\n",
      "random_forest\n",
      " [[794  37]\n",
      " [154  48]]\n"
     ]
    }
   ],
   "source": [
    "twitter_sg_confusion_matrix = {}\n",
    "for name, model in models.items():\n",
    "    y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "    twitter_sg_confusion_matrix[name] = confusion_matrix(y, y_pred)\n",
    "    print(name+'\\n',twitter_sg_confusion_matrix[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NILC skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining possible parameters for every classifier\n",
    "SVM = {'name': 'svm',\n",
    "       'classifier': SVC(),\n",
    "       'parameters': {'kernel': ['linear', 'rbf'],\n",
    "       'C': [1.0, 5, 10, 15],\n",
    "       'gamma': [0.1, 0.5, 1.0, 5.0, 10]}}\n",
    "\n",
    "Logistic = {'name': 'logistic_regression',\n",
    "            'classifier': LogisticRegression(),\n",
    "            'parameters': {'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1.0, 5.0, 10],\n",
    "            'max_iter': [100, 500, 1000],\n",
    "            'solver': ['liblinear']}}\n",
    "\n",
    "MLP = {'name': 'multilayer_perceptron',\n",
    "       'classifier': MLPClassifier(),\n",
    "       'parameters': {'hidden_layer_sizes':[(100,),(1000,),(100,100)],\n",
    "                      'solver': ['lbfgs']}}\n",
    "\n",
    "Random_Forest = {'name': 'random_forest',\n",
    "                 'classifier': RandomForestClassifier(),\n",
    "                 'parameters': {'n_estimators': [10, 50, 100, 300, 'warn']}}\n",
    "\n",
    "classifiers = [SVM, Logistic, MLP, Random_Forest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nilc_skipgram = KeyedVectors.load_word2vec_format('data/word_embeddings/skip_s100.txt')\n",
    "twitter_skipgram = gensim.models.Word2Vec.load('data/word_embeddings/twitter_skipgram_100_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_vectors = []\n",
    "for comment in comments:\n",
    "    comment_vector = np.zeros((100))\n",
    "    if not comment:\n",
    "        comments_vectors.append(comment_vector)\n",
    "    else:\n",
    "        n_tokens = 0\n",
    "        for token in comment:\n",
    "            n_tokens += 1\n",
    "            try:\n",
    "                comment_vector += nilc_skipgram.wv.get_vector(token)\n",
    "            except KeyError:\n",
    "                comment_vector += np.zeros((100))\n",
    "            \n",
    "        comments_vectors.append(comment_vector/n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments_vectors\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   11.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  25 | elapsed:    2.9s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    5.7s finished\n"
     ]
    }
   ],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=2,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.8267288446548646\n",
      "logistic_regression: 0.823001397999788\n",
      "multilayer_perceptron: 0.7950640926230416\n",
      "random_forest: 0.8031388320447483\n"
     ]
    }
   ],
   "source": [
    "nilc_sg_results = {}\n",
    "for name, model in models.items():\n",
    "    nilc_sg_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring=make_scorer(f1_score, average='weighted')))\n",
    "    \n",
    "for name, result in nilc_sg_results.items():\n",
    "    print(name+':', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      " [[799  32]\n",
      " [126  76]]\n",
      "logistic_regression\n",
      " [[771  60]\n",
      " [112  90]]\n",
      "multilayer_perceptron\n",
      " [[723 108]\n",
      " [112  90]]\n",
      "random_forest\n",
      " [[806  25]\n",
      " [144  58]]\n"
     ]
    }
   ],
   "source": [
    "nilc_sg_confusion_matrix = {}\n",
    "for name, model in models.items():\n",
    "    y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "    nilc_sg_confusion_matrix[name] = confusion_matrix(y, y_pred)\n",
    "    print(name+'\\n',nilc_sg_confusion_matrix[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = [bow_results, tf_idf_results, twitter_cbow_results,\n",
    "                 twitter_sg_results, nilc_cbow_results, nilc_sg_results]\n",
    "\n",
    "final_matrices = {'bow baseline': bow_confusion_matrix, \n",
    "                  'tf idf':tf_idf_confusion_matrix,\n",
    "                  'twitter cbow':twitter_cbow_confusion_matrix,\n",
    "                  'twitter skipgram':twitter_sg_confusion_matrix,\n",
    "                  'nilc cbow':nilc_cbow_confusion_matrix,\n",
    "                  'nilc skipgram':nilc_sg_confusion_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svm</th>\n",
       "      <th>logistic_regression</th>\n",
       "      <th>multilayer_perceptron</th>\n",
       "      <th>random_forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bow baseline</th>\n",
       "      <td>0.764108</td>\n",
       "      <td>0.782418</td>\n",
       "      <td>0.762470</td>\n",
       "      <td>0.775197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf idf</th>\n",
       "      <td>0.782145</td>\n",
       "      <td>0.770321</td>\n",
       "      <td>0.770636</td>\n",
       "      <td>0.762902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter cbow</th>\n",
       "      <td>0.806940</td>\n",
       "      <td>0.813692</td>\n",
       "      <td>0.747183</td>\n",
       "      <td>0.780117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter skipgram</th>\n",
       "      <td>0.820629</td>\n",
       "      <td>0.830169</td>\n",
       "      <td>0.770226</td>\n",
       "      <td>0.769113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nilc cbow</th>\n",
       "      <td>0.820772</td>\n",
       "      <td>0.818160</td>\n",
       "      <td>0.764893</td>\n",
       "      <td>0.787602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nilc skipgram</th>\n",
       "      <td>0.826729</td>\n",
       "      <td>0.823001</td>\n",
       "      <td>0.795064</td>\n",
       "      <td>0.803139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       svm  logistic_regression  multilayer_perceptron  \\\n",
       "bow baseline      0.764108             0.782418               0.762470   \n",
       "tf idf            0.782145             0.770321               0.770636   \n",
       "twitter cbow      0.806940             0.813692               0.747183   \n",
       "twitter skipgram  0.820629             0.830169               0.770226   \n",
       "nilc cbow         0.820772             0.818160               0.764893   \n",
       "nilc skipgram     0.826729             0.823001               0.795064   \n",
       "\n",
       "                  random_forest  \n",
       "bow baseline           0.775197  \n",
       "tf idf                 0.762902  \n",
       "twitter cbow           0.780117  \n",
       "twitter skipgram       0.769113  \n",
       "nilc cbow              0.787602  \n",
       "nilc skipgram          0.803139  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = {}\n",
    "for result in final_results:\n",
    "    for name, score in result.items():\n",
    "        if not df.get(name):\n",
    "            df[name] = [score]\n",
    "        else:\n",
    "            df[name].append(score)\n",
    "            \n",
    "pd.DataFrame(df, index=['bow baseline', 'tf idf', 'twitter cbow',\n",
    "                        'twitter skipgram', 'nilc cbow', 'nilc skipgram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow baseline\n",
      "svm\n",
      "        0 pred  1 pred\n",
      "0 true     719     112\n",
      "1 true     128      74\n",
      "\n",
      "logistic_regression\n",
      "        0 pred  1 pred\n",
      "0 true     775      56\n",
      "1 true     145      57\n",
      "\n",
      "multilayer_perceptron\n",
      "        0 pred  1 pred\n",
      "0 true     703     128\n",
      "1 true     121      81\n",
      "\n",
      "random_forest\n",
      "        0 pred  1 pred\n",
      "0 true     788      43\n",
      "1 true     156      46\n",
      "\n",
      "tf idf\n",
      "svm\n",
      "        0 pred  1 pred\n",
      "0 true     783      48\n",
      "1 true     149      53\n",
      "\n",
      "logistic_regression\n",
      "        0 pred  1 pred\n",
      "0 true     806      25\n",
      "1 true     167      35\n",
      "\n",
      "multilayer_perceptron\n",
      "        0 pred  1 pred\n",
      "0 true     731     100\n",
      "1 true     134      68\n",
      "\n",
      "random_forest\n",
      "        0 pred  1 pred\n",
      "0 true     780      51\n",
      "1 true     161      41\n",
      "\n",
      "twitter cbow\n",
      "svm\n",
      "        0 pred  1 pred\n",
      "0 true     765      66\n",
      "1 true     120      82\n",
      "\n",
      "logistic_regression\n",
      "        0 pred  1 pred\n",
      "0 true     765      66\n",
      "1 true     115      87\n",
      "\n",
      "multilayer_perceptron\n",
      "        0 pred  1 pred\n",
      "0 true     691     140\n",
      "1 true     125      77\n",
      "\n",
      "random_forest\n",
      "        0 pred  1 pred\n",
      "0 true     813      18\n",
      "1 true     162      40\n",
      "\n",
      "twitter skipgram\n",
      "svm\n",
      "        0 pred  1 pred\n",
      "0 true     786      45\n",
      "1 true     122      80\n",
      "\n",
      "logistic_regression\n",
      "        0 pred  1 pred\n",
      "0 true     775      56\n",
      "1 true     109      93\n",
      "\n",
      "multilayer_perceptron\n",
      "        0 pred  1 pred\n",
      "0 true     688     143\n",
      "1 true     108      94\n",
      "\n",
      "random_forest\n",
      "        0 pred  1 pred\n",
      "0 true     794      37\n",
      "1 true     154      48\n",
      "\n",
      "nilc cbow\n",
      "svm\n",
      "        0 pred  1 pred\n",
      "0 true     771      60\n",
      "1 true     114      88\n",
      "\n",
      "logistic_regression\n",
      "        0 pred  1 pred\n",
      "0 true     772      59\n",
      "1 true     117      85\n",
      "\n",
      "multilayer_perceptron\n",
      "        0 pred  1 pred\n",
      "0 true     707     124\n",
      "1 true     113      89\n",
      "\n",
      "random_forest\n",
      "        0 pred  1 pred\n",
      "0 true     818      13\n",
      "1 true     162      40\n",
      "\n",
      "nilc skipgram\n",
      "svm\n",
      "        0 pred  1 pred\n",
      "0 true     799      32\n",
      "1 true     126      76\n",
      "\n",
      "logistic_regression\n",
      "        0 pred  1 pred\n",
      "0 true     771      60\n",
      "1 true     112      90\n",
      "\n",
      "multilayer_perceptron\n",
      "        0 pred  1 pred\n",
      "0 true     723     108\n",
      "1 true     112      90\n",
      "\n",
      "random_forest\n",
      "        0 pred  1 pred\n",
      "0 true     806      25\n",
      "1 true     144      58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature_name, feature in final_matrices.items():\n",
    "    print(feature_name)\n",
    "    for name, matrix in feature.items():\n",
    "        print(name)\n",
    "        print(pd.DataFrame(matrix, columns=['0 pred', '1 pred'], index=['0 true', '1 true']))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
