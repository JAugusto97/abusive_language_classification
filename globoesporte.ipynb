{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/corpus/OffComBR/OffComBR3.arff\") as f:\n",
    "    dataDictionary = arff.load(f)\n",
    "    f.close()\n",
    "    \n",
    "globoesporte_data = pd.DataFrame(dataDictionary['data'], columns=['toxico', 'comentario'])\n",
    "globoesporte_data['toxico'] = globoesporte_data['toxico'].apply(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxico</th>\n",
       "      <th>comentario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Votaram no PEZAO Agora tomem no CZAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>cuidado com a poupanca pessoal Lembram o que a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Sabe o que eu acho engracado os nossos governa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Podiam retirar dos lucros dos bancos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>CADE O GALVAO PRA NARRAR AGORA   FALIIIIUUUUUU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Nao e possivel que deputados tenham tantos car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Sejam honestos aprovem o projeto original vamo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Nao a pizza anunciada de anistia do caixa  Sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>quem ja viu bandidos fazendo leis contra antic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Olha os caras fazendo algo de util e de intere...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxico                                         comentario\n",
       "0       1               Votaram no PEZAO Agora tomem no CZAO\n",
       "1       0  cuidado com a poupanca pessoal Lembram o que a...\n",
       "2       0  Sabe o que eu acho engracado os nossos governa...\n",
       "3       0              Podiam retirar dos lucros dos bancos \n",
       "4       0  CADE O GALVAO PRA NARRAR AGORA   FALIIIIUUUUUU...\n",
       "5       0  Nao e possivel que deputados tenham tantos car...\n",
       "6       0  Sejam honestos aprovem o projeto original vamo...\n",
       "7       0  Nao a pizza anunciada de anistia do caixa  Sim...\n",
       "8       0  quem ja viu bandidos fazendo leis contra antic...\n",
       "9       0  Olha os caras fazendo algo de util e de intere..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globoesporte_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxico</th>\n",
       "      <th>comentario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Votaram no PEZAO Agora tomem no CZAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>Voces sao idiotas ou se fazem Voces sabem que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>Alexandre Acioni vai estudar pra saber o que e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>Mais um pobre metido a besta ja ja fica sem di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>Martin Sales deixa de ser idiot  agora ninguem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>nem sou mas esse cara e um velho asqueroso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>A FMI nao manda nas economia mundiais Que vao ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>PAULO VOCE E BURRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>VAO BATE PANELASSEUS BURROSBEM FEITO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>ninguem se importa com vc nem sua mae seu nada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     toxico                                         comentario\n",
       "0         1               Votaram no PEZAO Agora tomem no CZAO\n",
       "25        1  Voces sao idiotas ou se fazem Voces sabem que ...\n",
       "31        1  Alexandre Acioni vai estudar pra saber o que e...\n",
       "42        1  Mais um pobre metido a besta ja ja fica sem di...\n",
       "46        1  Martin Sales deixa de ser idiot  agora ninguem...\n",
       "60        1         nem sou mas esse cara e um velho asqueroso\n",
       "66        1  A FMI nao manda nas economia mundiais Que vao ...\n",
       "83        1                                 PAULO VOCE E BURRO\n",
       "84        1               VAO BATE PANELASSEUS BURROSBEM FEITO\n",
       "148       1     ninguem se importa com vc nem sua mae seu nada"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globoesporte_data[globoesporte_data['toxico'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating different classifiers with different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining possible parameters for every classifier\n",
    "SVM = {'name': 'svm',\n",
    "       'classifier': SVC(),\n",
    "       'parameters': {'kernel': ['linear'],\n",
    "       'C': [0.1, 1.0, 5, 10, 15],\n",
    "       'gamma': [0.01, 0.1, 1.0, 5.0, 10],}}\n",
    "\n",
    "Logistic = {'name': 'logistic_regression',\n",
    "            'classifier': LogisticRegression(),\n",
    "            'parameters': {'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1.0, 5.0, 10],\n",
    "            'solver': ['liblinear']}}\n",
    "\n",
    "MLP = {'name': 'multilayer_perceptron',\n",
    "       'classifier': MLPClassifier(),\n",
    "       'parameters': {'hidden_layer_sizes':[(100,),(100,100,),(1000,)],\n",
    "                      'solver': ['lbfgs']}}\n",
    "\n",
    "Random_Forest = {'name': 'random_forest',\n",
    "                 'classifier': RandomForestClassifier(),\n",
    "                 'parameters': {'n_estimators': [10, 50, 100, 300]}}\n",
    "\n",
    "classifiers = [SVM, Logistic, MLP, Random_Forest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf - idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer().fit(globoesporte_data['comentario'])\n",
    "X = vectorizer.transform(globoesporte_data['comentario'])\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=0,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.3454398708635997\n",
      "logistic_regression: 0.2633626027243049\n",
      "multilayer_perceptron: 0.4448896426500644\n",
      "random_forest: 0.29695653366241603\n"
     ]
    }
   ],
   "source": [
    "tf_idf_results = {}\n",
    "for name, model in models.items():\n",
    "    tf_idf_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring='f1'))\n",
    "    \n",
    "for name, result in tf_idf_results.items():\n",
    "    print(name+':', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embeddings with size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating array of tokens for every comment\n",
    "comments = []\n",
    "STOPWORDS = nltk.corpus.stopwords.words('portuguese')\n",
    "for i, comment in globoesporte_data['comentario'].iteritems():\n",
    "    tokenized_comment = nltk.word_tokenize(comment.lower())\n",
    "    \n",
    "    clean_comment = [token for token in tokenized_comment if\n",
    "                     len(token) > 3 and\n",
    "                     token not in STOPWORDS and\n",
    "                     token.isalpha()]\n",
    "    \n",
    "    comments.append(clean_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = gensim.models.Word2Vec.load('data/word_embeddings/twitter_cbow_100_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging vectors of tokens into vectors of comments\n",
    "comments_vectors = []\n",
    "for comment in comments:\n",
    "    comment_vector = np.zeros((100))\n",
    "    n_tokens = 1\n",
    "    for token in comment:\n",
    "        n_tokens += 1\n",
    "        try:\n",
    "            comment_vector += cbow.wv.get_vector(token)\n",
    "        except KeyError:\n",
    "            comment_vector += np.zeros((100))\n",
    "            \n",
    "    comments_vectors.append(comment_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments_vectors\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=0,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.3265405279387922\n",
      "logistic_regression: 0.372000762955991\n",
      "multilayer_perceptron: 0.36884622972906933\n",
      "random_forest: 0.34907225754040155\n"
     ]
    }
   ],
   "source": [
    "twitter_cbow_results = {}\n",
    "for name, model in models.items():\n",
    "    twitter_cbow_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring='f1'))\n",
    "    \n",
    "for name, result in twitter_cbow_results.items():\n",
    "    print(name+':', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NILC Cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "cbow = KeyedVectors.load_word2vec_format('data/word_embeddings/cbow_s100.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_vectors = []\n",
    "for comment in comments:\n",
    "    comment_vector = np.zeros((100))\n",
    "    if not comment:\n",
    "        comments_vectors.append(comment_vector)\n",
    "    else:\n",
    "        n_tokens = 0\n",
    "        for token in comment:\n",
    "            n_tokens += 1\n",
    "            try:\n",
    "                comment_vector += cbow.wv.get_vector(token)\n",
    "            except KeyError:\n",
    "                comment_vector += np.zeros((100))\n",
    "            \n",
    "        comments_vectors.append(comment_vector/n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments_vectors\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=0,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.47301366733624794\n",
      "logistic_regression: 0.48084587860068967\n",
      "multilayer_perceptron: 0.401715864695998\n",
      "random_forest: 0.2912405585146399\n"
     ]
    }
   ],
   "source": [
    "nilc_cbow_results = {}\n",
    "for name, model in models.items():\n",
    "    nilc_cbow_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring='f1'))\n",
    "    \n",
    "for name, result in nilc_cbow_results.items():\n",
    "    print(name+':', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram = gensim.models.Word2Vec.load('data/word_embeddings/twitter_skipgram_100_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_vectors = []\n",
    "for comment in comments:\n",
    "    comment_vector = np.zeros((100))\n",
    "    if not comment:\n",
    "        comments_vectors.append(comment_vector)\n",
    "    else:\n",
    "        n_tokens = 0\n",
    "        for token in comment:\n",
    "            n_tokens += 1\n",
    "            try:\n",
    "                comment_vector += skipgram.wv.get_vector(token)\n",
    "            except KeyError:\n",
    "                comment_vector += np.zeros((100))\n",
    "            \n",
    "        comments_vectors.append(comment_vector/n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments_vectors\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=0,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.48511671480547386\n",
      "logistic_regression: 0.5314671981051043\n",
      "multilayer_perceptron: 0.44203692436588443\n",
      "random_forest: 0.3719229078161234\n"
     ]
    }
   ],
   "source": [
    "twitter_sg_results = {}\n",
    "for name, model in models.items():\n",
    "    twitter_sg_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring='f1'))\n",
    "    \n",
    "for name, result in twitter_sg_results.items():\n",
    "    print(name+':', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NILC skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram = KeyedVectors.load_word2vec_format('data/word_embeddings/skip_s100.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_vectors = []\n",
    "for comment in comments:\n",
    "    comment_vector = np.zeros((100))\n",
    "    if not comment:\n",
    "        comments_vectors.append(comment_vector)\n",
    "    else:\n",
    "        n_tokens = 0\n",
    "        for token in comment:\n",
    "            n_tokens += 1\n",
    "            try:\n",
    "                comment_vector += skipgram.wv.get_vector(token)\n",
    "            except KeyError:\n",
    "                comment_vector += np.zeros((100))\n",
    "            \n",
    "        comments_vectors.append(comment_vector/n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments_vectors\n",
    "y = globoesporte_data['toxico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Grid Search to find the best model for each classifier\n",
    "models = {}\n",
    "for classifier in classifiers:\n",
    "    gs = GridSearchCV(estimator=classifier['classifier'],\n",
    "                                param_grid=classifier['parameters'],\n",
    "                                scoring='f1',\n",
    "                                verbose=0,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    models[classifier['name']] = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.48328831030009045\n",
      "logistic_regression: 0.5070986359786197\n",
      "multilayer_perceptron: 0.42813713084947896\n",
      "random_forest: 0.4024322427148882\n"
     ]
    }
   ],
   "source": [
    "nilc_sg_results = {}\n",
    "for name, model in models.items():\n",
    "    nilc_sg_results[name] = np.average(cross_val_score(model,\n",
    "                                            X,\n",
    "                                            y,\n",
    "                                            cv=5,\n",
    "                                            scoring='f1'))\n",
    "    \n",
    "for name, result in nilc_sg_results.items():\n",
    "    print(name+':', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = [tf_idf_results, twitter_cbow_results,\n",
    "                 twitter_sg_results, nilc_cbow_results,\n",
    "                 nilc_sg_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svm</th>\n",
       "      <th>logistic_regression</th>\n",
       "      <th>multilayer_perceptron</th>\n",
       "      <th>random_forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tf-idf</th>\n",
       "      <td>0.345440</td>\n",
       "      <td>0.263363</td>\n",
       "      <td>0.444890</td>\n",
       "      <td>0.296957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter_cbow</th>\n",
       "      <td>0.326541</td>\n",
       "      <td>0.372001</td>\n",
       "      <td>0.368846</td>\n",
       "      <td>0.349072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter_skipgram</th>\n",
       "      <td>0.485117</td>\n",
       "      <td>0.531467</td>\n",
       "      <td>0.442037</td>\n",
       "      <td>0.371923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nilc_cbow</th>\n",
       "      <td>0.473014</td>\n",
       "      <td>0.480846</td>\n",
       "      <td>0.401716</td>\n",
       "      <td>0.291241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nilc_skipgram</th>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.507099</td>\n",
       "      <td>0.428137</td>\n",
       "      <td>0.402432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       svm  logistic_regression  multilayer_perceptron  \\\n",
       "tf-idf            0.345440             0.263363               0.444890   \n",
       "twitter_cbow      0.326541             0.372001               0.368846   \n",
       "twitter_skipgram  0.485117             0.531467               0.442037   \n",
       "nilc_cbow         0.473014             0.480846               0.401716   \n",
       "nilc_skipgram     0.483288             0.507099               0.428137   \n",
       "\n",
       "                  random_forest  \n",
       "tf-idf                 0.296957  \n",
       "twitter_cbow           0.349072  \n",
       "twitter_skipgram       0.371923  \n",
       "nilc_cbow              0.291241  \n",
       "nilc_skipgram          0.402432  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = {}\n",
    "for result in final_results:\n",
    "    for name, score in result.items():\n",
    "        if not df.get(name):\n",
    "            df[name] = [score]\n",
    "        else:\n",
    "            df[name].append(score)\n",
    "            \n",
    "pd.DataFrame(df, index=['tf-idf', 'twitter_cbow','twitter_skipgram','nilc_cbow', 'nilc_skipgram'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
